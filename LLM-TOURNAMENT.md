# ğŸ† LLM Tournament & AI Agent Hiring

<div align="center">

**8 LLMs. 5 Days. 1 Blind Judge. $0.59 Total.**

*Can a $0.01/call API beat a $3.00/call premium model at thought leadership?*

**Spoiler: Yes. By 7 points. Then we used it to hire AI agents.**

[![DeepSeek](https://img.shields.io/badge/Winner-DeepSeek-gold?style=for-the-badge)](outputs/day-04/deepseek.md)
[![Score](https://img.shields.io/badge/Peak_Score-93%2F100-brightgreen?style=for-the-badge)](outputs/day-04/deepseek.md)
[![Agents Hired](https://img.shields.io/badge/Agents_Hired-3-purple?style=for-the-badge)](#-agent-hiring-process)
[![Cost](https://img.shields.io/badge/Total_Cost-$0.59-blue?style=for-the-badge)](#cost-analysis)

</div>

---

<details open>
<summary><h2>ğŸ¤– Agent Hiring Process</h2></summary>

> **How we built a security team of AI agents using LLMs to evaluate LLM-generated applications**

After the tournament proved DeepSeek's quality, we used it to generate job applications for AI agent rolesâ€”then had another LLM (Dr. Zero) evaluate them BLIND.

### The Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     AGENT HIRING PIPELINE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Job Post â”€â”€â–¶ Round 1 â”€â”€â–¶ Dr. Zero â”€â”€â–¶ Feedback â”€â”€â–¶ Round 2     â”‚
â”‚              (Zero-Shot)   (BLIND)      Loop        (Revised)    â”‚
â”‚                                                                  â”‚
â”‚                              â”‚                         â”‚         â”‚
â”‚                              â–¼                         â–¼         â”‚
â”‚                         Score < 85              Score â‰¥ 85       â”‚
â”‚                         PROBATION                  HIRE          â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Current Team

| Agent | Codename | Role | Score | Rounds | Cost |
|-------|----------|------|:-----:|:------:|:----:|
| ğŸ›¡ï¸ IAM Agent | **GUARDIAN** | Cloud Identity Security | 92/100 | 2 | $0.04 |
| ğŸ” Threat Intel | **HUNTER** | Threat Intelligence | 92/100 | 2 | $0.04 |
| ğŸ”´ Red Team | **PHOENIX** | Attack Simulation | 89/100 | 2 | $0.04 |

**Total hiring cost: $0.12** for a complete security team.

### Featured: GUARDIAN's Journey

<details>
<summary>ğŸ“„ Round 1: Score 78 (PROBATION)</summary>

**Strengths:**
- Comprehensive multi-cloud coverage (AWS, Azure, GCP)
- Strong privilege escalation knowledge (21+ techniques)
- Good "Not" statement trap awareness

**Dr. Zero's Feedback:**
> "The application reads like 2024 IAM expertise applied to 2026 problems. I need 2026 IAM expertise."

**Gaps:**
- AI agent identity mentioned but not deep
- No CVE awareness
- Team integration was a list, not workflows

</details>

<details>
<summary>ğŸ“„ Round 2: Score 92 (EXCEPTIONAL HIRE)</summary>

**Improvements (+14 points):**
- Deep AI agent identity section with credential models, permission scoping
- CVE awareness (CVE-2025-55241, CVE-2026-24305)
- Real team integration workflows
- Self-improvement system with ML techniques
- Tooling ecosystem knowledge (15+ tools)

**Dr. Zero's Verdict:**
> "+14 points in one round. That's the kind of learning rate that makes a great agent. GUARDIAN is hired."

</details>

### The Gap GUARDIAN Fills

Most organizations lack a dedicated cloud identity security role:

| Team | Focus | IAM Blind Spot |
|------|-------|----------------|
| **Identity Team** | IdP (Entra, Okta), IGA (SailPoint) | Cloud IAM policies |
| **Platform Team** | Infrastructure, CI/CD | Least privilege |
| **AppSec Team** | SAST, DAST, dependencies | IAM deprioritized |

**GUARDIAN fills this gap as a dedicated cloud identity security specialist.**

### Evaluation Rubric

| Dimension | 18-20 | 14-17 | 10-13 |
|-----------|-------|-------|-------|
| **Technical Depth** | Specific CVEs, attack chains | Real expertise | Surface-level |
| **AI/Agentic Focus** | Deep agent identity expertise | Good awareness | Mentioned |
| **Team Integration** | Real workflows | Good collaboration | Listed skills |

**Hire threshold: 85+** | **Probation: 78-84** | **Reject: <78**

---

ğŸ“ **[Full agent hiring documentation â†’](agent-hiring/)**

</details>

---

<details open>
<summary><h2>ğŸ“Š LLM Tournament Results</h2></summary>

```
                           FINAL STANDINGS
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ¥‡ DeepSeek      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  85.4  â”‚
    â”‚  ğŸ¥ˆ Anthropic     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘  78.2  â”‚
    â”‚  ğŸ¥‰ Vertex        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘  77.2  â”‚
    â”‚  4. Azure         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  73.0  â”‚
    â”‚  5. Qwen (local)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  72.2  â”‚
    â”‚  6. OpenAI        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  71.0  â”‚
    â”‚  7. Ollama (local)â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  68.2  â”‚
    â”‚  8. Groq          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  67.6  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Rank | Provider | Model | D1 | D2 | D3 | D4 | D5 | Avg | Peak |
|:----:|----------|-------|:--:|:--:|:--:|:--:|:--:|:---:|:----:|
| ğŸ¥‡ | **DeepSeek** | deepseek-chat | 75 | 88 | 86 | **93** | 85 | **85.4** | 93 |
| ğŸ¥ˆ | **Anthropic** | claude-sonnet-4 | 75 | 84 | 78 | 78 | 76 | 78.2 | 84 |
| ğŸ¥‰ | **Vertex** | gemini-2.0-flash | 68 | 76 | 82 | 85 | 75 | 77.2 | 85 |
| 4 | Azure | gpt-4o-mini | 65 | 65 | 78 | 82 | 75 | 73.0 | 82 |
| 5 | Qwen | qwen2.5:32b (local) | 65 | 70 | 78 | 76 | 72 | 72.2 | 78 |
| 6 | OpenAI | gpt-4o-mini | 68 | 74 | 66 | 73 | 74 | 71.0 | 74 |
| 7 | Ollama | llama3.1:8b (local) | 65 | 62 | 67 | 75 | 72 | 68.2 | 75 |
| 8 | Groq | llama-3.3-70b | 55 | 66 | 72 | 70 | 75 | 67.6 | 75 |

</details>

---

<details>
<summary><h2>ğŸ¯ The Challenge</h2></summary>

> Write a LinkedIn post about Non-Human Identities in cloud security that a CISO would actually share.

Each model got:
- Same knowledge base (5 trends, 5 predictions)
- Same system prompt
- Same evaluation criteria
- **Blind judge** (no idea which model wrote what)

</details>

---

<details>
<summary><h2>ğŸ“ˆ The Evolution</h2></summary>

**How does a model go from 75 to 93 in 4 iterations?**

<table>
<tr>
<td width="50%">

### Day 1: Score 75 âŒ
```
"The 100:1 ratio is coming..."
```
Generic opener. Vague advice. No proof.

</td>
<td width="50%">

### Day 4: Score 93 âœ…
```
"Your 2026 SOC will be overwhelmed 
by alerts from employees you 
never hired."
```
Visceral hook. CVE citation. Board-level framing.

</td>
</tr>
</table>

**[ğŸ“– See the full annotated evolution â†’](DEEPSEEK-EVOLUTION.md)**

</details>

---

<details>
<summary><h2>ğŸ”¬ The Method</h2></summary>

We used **In-Context Learning (ICL)** - feeding each model its previous feedback:

```
Day 1: Zero-shot      â†’ Baseline capability
Day 2: +Feedback      â†’ Can it learn?
Day 3: +Tuning        â†’ Optimal parameters?
Day 4: +Winner shown  â†’ Can it learn from the best?
Day 5: +Everything    â†’ Does more context help?
```

**Key Finding:** Day 4 was optimal. Day 5 showed *overfitting* - too much context hurt performance.

</details>

---

<details>
<summary><h2>ğŸ’° Cost Analysis</h2></summary>

| Provider | Quality | Cost/Call | Monthly (1K calls) | Value |
|----------|:-------:|:---------:|:------------------:|:-----:|
| **DeepSeek** | 85.4 | $0.01 | $10 | ğŸ† Best |
| Qwen | 72.2 | $0.00 | $0 | ğŸ† Best Free |
| Anthropic | 78.2 | $0.01 | $10 | Good |
| Azure | 73.0 | $0.05 | $50 | âŒ Poor |

**The $0.01 API beat the $3.00 API.** Price â‰  Quality.

</details>

---

<details>
<summary><h2>âš¡ Speed vs Quality</h2></summary>

```
                    HIGH QUALITY
                         â”‚
         DeepSeek â˜…      â”‚
              (85)       â”‚      
                         â”‚
    Anthropic â˜…          â”‚      â˜… Vertex (77)
        (78)             â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         SLOW            â”‚           FAST
                         â”‚
                         â”‚      â˜… Groq (68)
    Qwen â˜…               â”‚        1.5s avg
     (72)                â”‚
     35s avg             â”‚
                         â”‚
                    LOW QUALITY
```

**Need speed?** Groq (1.5s) 
**Need quality?** DeepSeek (14s)
**Need both?** Vertex (4s, 77 score)

</details>

---

## ğŸ—‚ï¸ What's In This Repo

```
â”œâ”€â”€ ğŸ“Š FINAL-REPORT.md        # Complete tournament analysis
â”œâ”€â”€ ğŸ“ˆ DEEPSEEK-EVOLUTION.md  # Day 1â†’4 annotated comparison
â”œâ”€â”€ ğŸ”¬ METHODOLOGY.md         # Experimental design
â”œâ”€â”€ ğŸ¤– agent-hiring/          # AI agent hiring process â­ NEW
â”‚   â”œâ”€â”€ README.md             # Full hiring documentation
â”‚   â”œâ”€â”€ guardian/             # IAM Agent (GUARDIAN)
â”‚   â”œâ”€â”€ hunter/               # Threat Intel (HUNTER)
â”‚   â””â”€â”€ evaluation-rubric.md  # Scoring criteria
â”œâ”€â”€ ğŸ“ outputs/
â”‚   â”œâ”€â”€ day-01/               # Zero-shot baseline
â”‚   â”œâ”€â”€ day-02/               # First feedback
â”‚   â”œâ”€â”€ day-03/               # Parameter tuning
â”‚   â”œâ”€â”€ day-04/               # Peak performance â­
â”‚   â”œâ”€â”€ day-05/               # Overfitting observed
â”‚   â””â”€â”€ day-06/               # Per-model optimization (bonus)
â””â”€â”€ ğŸ“‰ charts/
    â””â”€â”€ trajectory.svg        # Score progression
```

---

## ğŸ“ Key Takeaways

### From the Tournament
1. **Price â‰  Quality** - DeepSeek ($0.14/1M) beat Anthropic ($3.00/1M) by 7 points
2. **Feedback Works** - 18% average improvement from Day 1 to Day 4
3. **Know When to Stop** - 3-4 iterations optimal, more causes overfitting
4. **Local Models Are Viable** - Qwen 32B (free) scored 72.2

### From Agent Hiring
5. **BLIND evaluation eliminates bias** - Dr. Zero didn't know which LLM generated applications
6. **Learning rate matters** - GUARDIAN improved +14 points in one round
7. **Agents can fill organizational gaps** - GUARDIAN fills the cloud identity gap between Identity, Platform, and AppSec teams

---

## âš ï¸ Limitations

This is a **practical benchmark**, not a peer-reviewed study:

- Single evaluator (Azure GPT-4o-mini)
- Small sample size (n=5 per model)
- Same topic throughout
- Infrastructure variance

**[See proposed follow-up studies â†’](FINAL-REPORT.md#whats-next)**

---

## ğŸš€ Try It Yourself

```bash
# Clone and explore
git clone https://github.com/colehorsman/LLM-Tournament.git
cd LLM-Tournament

# Read the winner's peak performance
cat outputs/day-04/deepseek.md

# Explore the agent hiring process
cat agent-hiring/README.md

# See GUARDIAN's journey from 78 to 92
cat agent-hiring/guardian/review-v2.md
```

---

## ğŸ“ License

MIT - Use these results however you'd like. Attribution appreciated.

---

<div align="center">

**Built with the [Agentic Research Platform](https://github.com/colehorsman/agentic-research-platform)**

*Tournament run: February 2, 2026*

---

*Questions? Open an issue or reach out on [LinkedIn](https://linkedin.com/in/colehorsman)*

</div>
