# ğŸ† LLM Tournament: Who Writes Better Security Content?

<div align="center">

**8 LLMs. 5 Days. 1 Blind Judge. $0.59 Total.**

*Can a $0.01/call API beat a $3.00/call premium model at thought leadership?*

**Spoiler: Yes. By 7 points.**

[![DeepSeek](https://img.shields.io/badge/Winner-DeepSeek-gold?style=for-the-badge)](outputs/day-04/deepseek.md)
[![Score](https://img.shields.io/badge/Peak_Score-93%2F100-brightgreen?style=for-the-badge)](outputs/day-04/deepseek.md)
[![Cost](https://img.shields.io/badge/Total_Cost-$0.59-blue?style=for-the-badge)](#cost-analysis)

</div>

---

## ğŸ“Š The Results

```
                           FINAL STANDINGS
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ¥‡ DeepSeek      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  85.4  â”‚
    â”‚  ğŸ¥ˆ Anthropic     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘  78.2  â”‚
    â”‚  ğŸ¥‰ Vertex        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘  77.2  â”‚
    â”‚  4. Azure         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  73.0  â”‚
    â”‚  5. Qwen (local)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  72.2  â”‚
    â”‚  6. OpenAI        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  71.0  â”‚
    â”‚  7. Ollama (local)â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  68.2  â”‚
    â”‚  8. Groq          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  67.6  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Rank | Provider | Model | D1 | D2 | D3 | D4 | D5 | Avg | Peak |
|:----:|----------|-------|:--:|:--:|:--:|:--:|:--:|:---:|:----:|
| ğŸ¥‡ | **DeepSeek** | deepseek-chat | 75 | 88 | 86 | **93** | 85 | **85.4** | 93 |
| ğŸ¥ˆ | **Anthropic** | claude-sonnet-4 | 75 | 84 | 78 | 78 | 76 | 78.2 | 84 |
| ğŸ¥‰ | **Vertex** | gemini-2.0-flash | 68 | 76 | 82 | 85 | 75 | 77.2 | 85 |
| 4 | Azure | gpt-4o-mini | 65 | 65 | 78 | 82 | 75 | 73.0 | 82 |
| 5 | Qwen | qwen2.5:32b (local) | 65 | 70 | 78 | 76 | 72 | 72.2 | 78 |
| 6 | OpenAI | gpt-4o-mini | 68 | 74 | 66 | 73 | 74 | 71.0 | 74 |
| 7 | Ollama | llama3.1:8b (local) | 65 | 62 | 67 | 75 | 72 | 68.2 | 75 |
| 8 | Groq | llama-3.3-70b | 55 | 66 | 72 | 70 | 75 | 67.6 | 75 |

---

## ğŸ¯ The Challenge

> Write a LinkedIn post about Non-Human Identities in cloud security that a CISO would actually share.

Each model got:
- Same knowledge base (5 trends, 5 predictions)
- Same system prompt
- Same evaluation criteria
- **Blind judge** (no idea which model wrote what)

---

## ğŸ“ˆ The Evolution

**How does a model go from 75 to 93 in 4 iterations?**

<table>
<tr>
<td width="50%">

### Day 1: Score 75 âŒ
```
"The 100:1 ratio is coming..."
```
Generic opener. Vague advice. No proof.

</td>
<td width="50%">

### Day 4: Score 93 âœ…
```
"Your 2026 SOC will be overwhelmed 
by alerts from employees you 
never hired."
```
Visceral hook. CVE citation. Board-level framing.

</td>
</tr>
</table>

**[ğŸ“– See the full annotated evolution â†’](DEEPSEEK-EVOLUTION.md)**

---

## ğŸ”¬ The Method

We used **In-Context Learning (ICL)** - feeding each model its previous feedback:

```
Day 1: Zero-shot      â†’ Baseline capability
Day 2: +Feedback      â†’ Can it learn?
Day 3: +Tuning        â†’ Optimal parameters?
Day 4: +Winner shown  â†’ Can it learn from the best?
Day 5: +Everything    â†’ Does more context help?
```

**Key Finding:** Day 4 was optimal. Day 5 showed *overfitting* - too much context hurt performance.

---

## ğŸ’° Cost Analysis

| Provider | Quality | Cost/Call | Monthly (1K calls) | Value |
|----------|:-------:|:---------:|:------------------:|:-----:|
| **DeepSeek** | 85.4 | $0.01 | $10 | ğŸ† Best |
| Qwen | 72.2 | $0.00 | $0 | ğŸ† Best Free |
| Anthropic | 78.2 | $0.01 | $10 | Good |
| Azure | 73.0 | $0.05 | $50 | âŒ Poor |

**The $0.01 API beat the $3.00 API.** Price â‰  Quality.

---

## âš¡ Speed vs Quality

```
                    HIGH QUALITY
                         â”‚
         DeepSeek â˜…      â”‚
              (85)       â”‚      
                         â”‚
    Anthropic â˜…          â”‚      â˜… Vertex (77)
        (78)             â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         SLOW            â”‚           FAST
                         â”‚
                         â”‚      â˜… Groq (68)
    Qwen â˜…               â”‚        1.5s avg
     (72)                â”‚
     35s avg             â”‚
                         â”‚
                    LOW QUALITY
```

**Need speed?** Groq (1.5s) 
**Need quality?** DeepSeek (14s)
**Need both?** Vertex (4s, 77 score)

---

## ğŸ—‚ï¸ What's In This Repo

```
â”œâ”€â”€ ğŸ“Š FINAL-REPORT.md        # Complete analysis
â”œâ”€â”€ ğŸ“ˆ DEEPSEEK-EVOLUTION.md  # Day 1â†’4 annotated comparison
â”œâ”€â”€ ğŸ”¬ METHODOLOGY.md         # Experimental design
â”œâ”€â”€ ğŸ“ outputs/
â”‚   â”œâ”€â”€ day-01/               # Zero-shot baseline
â”‚   â”œâ”€â”€ day-02/               # First feedback
â”‚   â”œâ”€â”€ day-03/               # Parameter tuning
â”‚   â”œâ”€â”€ day-04/               # Peak performance â­
â”‚   â”œâ”€â”€ day-05/               # Overfitting observed
â”‚   â””â”€â”€ day-06/               # Per-model optimization (bonus)
â””â”€â”€ ğŸ“‰ charts/
    â””â”€â”€ trajectory.svg        # Score progression
```

**Each day folder contains:**
- `{provider}.md` - Full response + blind evaluation
- `results.json` - Scores, latency, tokens, cost
- `SUMMARY.md` - Day analysis

---

## ğŸ“ Key Takeaways

### 1. Price â‰  Quality
DeepSeek ($0.14/1M tokens) beat Anthropic ($3.00/1M tokens) by 7 points.

### 2. Feedback Works
18% average improvement from Day 1 to Day 4 across all models.

### 3. Know When to Stop
3-4 feedback iterations optimal. More causes overfitting.

### 4. Local Models Are Viable
Qwen 32B (free, local) scored 72.2 - competitive with paid APIs.

### 5. Tune Per Model
Day 6 showed Vertex could recover +10 points with custom parameters.

---

## âš ï¸ Limitations

This is a **practical benchmark**, not a peer-reviewed study:

- Single evaluator (Azure GPT-4o-mini)
- Small sample size (n=5 per model)
- Same topic throughout
- Infrastructure variance

**[See proposed follow-up studies â†’](FINAL-REPORT.md#whats-next)**

---

## ğŸš€ Try It Yourself

```bash
# Clone and explore
git clone https://github.com/colehorsman/LLM-Tournament.git
cd LLM-Tournament

# Read the winner's peak performance
cat outputs/day-04/deepseek.md

# Compare Day 1 vs Day 4
diff outputs/day-01/deepseek.md outputs/day-04/deepseek.md
```

---

## ğŸ“ License

MIT - Use these results however you'd like. Attribution appreciated.

---

<div align="center">

**Built with the [Agentic Research Platform](https://github.com/colehorsman/agentic-research-platform)**

*Tournament run: February 2, 2026*

---

*Questions? Open an issue or reach out on [LinkedIn](https://linkedin.com/in/colehorsman)*

</div>
